{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":" <font size = 1000px color = 'royalblue'><center><b>Pima Indians Diabetes<b></center></font>\n ","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.094767,"end_time":"2021-08-03T10:37:53.678704","exception":false,"start_time":"2021-08-03T10:37:53.583937","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"<hr>\n<p>\n- <font size=\"4px\"><b> Context:</b></font>\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n- <font size=\"4px\"><b> Problem Statement:</b></font>\n\nBuild a model to accurately predict whether the patients in the dataset have diabetes or not?\n\n- <font size=\"4px\"><b> Context: </b></font>\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n\n1. <b>Pregnancies:</b> Number of times pregnant\n\n2. <b>Glucose:</b> Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n\n3. <b>BloodPressure:</b> Diastolic blood pressure (mm Hg)\n\n4. <b>SkinThickness:</b> Triceps skin fold thickness (mm)\n\n5. <b>Insulin:</b> 2-Hour serum insulin (mu U/ml)\n\n6. <b>BMI:</b> Body mass index (weight in kg/(height in m)^2)\n\n7. <b>DiabetesPedigreeFunction:</b> Diabetes pedigree function\n\n8. <b>Age:</b> Age (years)\n\n9. <b>Outcome:</b> Class variable (0 or 1) 268 of 768 are 1, the others are 0\n\n- <font size=\"4px\"><b> Work Flow: </b></font>\n\n1. Acquire the Data\n2. Data Exploration\n3. Check For Correlation Among Features and Between Features and Target\n4. Creating New Features That May Impact Significance in prediction\n5. Data Cleaning and Preprocessing/ Feature Engineering\n6. Feature Selection using Random Forest\n7. Evaluating Different Models\n8. Selecting Model With Best Score And Predicting the unseen Test data to Model\n\n<p>\n\n    \n    \n    \n<hr>\n","metadata":{}},{"cell_type":"markdown","source":"<font color = \"#191970\" size=6px><center><b>Import Required Libraries</b></font>","metadata":{}},{"cell_type":"code","source":"#!pip install lightgbm","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:00.541127Z","iopub.execute_input":"2021-09-27T11:08:00.542271Z","iopub.status.idle":"2021-09-27T11:08:00.564387Z","shell.execute_reply.started":"2021-09-27T11:08:00.542139Z","shell.execute_reply":"2021-09-27T11:08:00.562947Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Data Wrangling and Data Analysis\n\nimport pandas as pd , numpy as np\n\n# Visualization\n\nfrom matplotlib import pyplot as plt, style\nimport matplotlib\nimport seaborn as sns\nimport plotly\n\n\n\n# Feature Engineering / Feature Selection\n\nfrom sklearn.feature_selection import  VarianceThreshold\nfrom sklearn import model_selection\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport scipy.stats as stats\nfrom sklearn import base\nimport optuna\nfrom optuna.integration import LightGBMPruningCallback\nfrom optuna import Trial, visualization\nfrom functools import partial\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom optuna.integration import OptunaSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n# Model Building\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn import metrics\n\n# Ignore Warings\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:00.566294Z","iopub.execute_input":"2021-09-27T11:08:00.567128Z","iopub.status.idle":"2021-09-27T11:08:04.013856Z","shell.execute_reply.started":"2021-09-27T11:08:00.567096Z","shell.execute_reply":"2021-09-27T11:08:04.013087Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"#191970\" size=6px><center><b>1. Data Collection</b></font>","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"../input/pima-indian-diabetes/health care diabetes.csv\")\n\ndf.head(5)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:04.014902Z","iopub.execute_input":"2021-09-27T11:08:04.015481Z","iopub.status.idle":"2021-09-27T11:08:04.063347Z","shell.execute_reply.started":"2021-09-27T11:08:04.015448Z","shell.execute_reply":"2021-09-27T11:08:04.062637Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"#191970\" size=6px><center><b>2. Data Cleaning and EDA</b></font>","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=5px><b>2.1 lets check shape of data</b></font>","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:04.064316Z","iopub.execute_input":"2021-09-27T11:08:04.064858Z","iopub.status.idle":"2021-09-27T11:08:04.071447Z","shell.execute_reply.started":"2021-09-27T11:08:04.064825Z","shell.execute_reply":"2021-09-27T11:08:04.070611Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=5px><b>2.2 lets check Summary Statisrics of the data</b></font>","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:04.073885Z","iopub.execute_input":"2021-09-27T11:08:04.074647Z","iopub.status.idle":"2021-09-27T11:08:04.133982Z","shell.execute_reply.started":"2021-09-27T11:08:04.074612Z","shell.execute_reply":"2021-09-27T11:08:04.132817Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>How can <b>Glucose</b>, <b>Blood Pressure</b>, <b>Skin Thickness</b>, <b>Insulin</b> and <b>BMI be <b>Zero</b> ?</font>\n* <font color = \"red\" size=4.8px>There Must be <b>Missing Value</b> Indicating <b>Zero</b> for Particular Entry, Lets Make those Entry as <b>Nan</b> for applying Imputation Techniques</font>\n* <font color = \"red\" size=4.8px>Looks Data is Skewed for Insulin , lets Confirm by Visualization</font>    ","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=5px><b>2.3 lets check for Distribution of Data</b></font>","metadata":{}},{"cell_type":"code","source":"style.use('classic')\n\na = 3  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\nplt.figure(figsize=(30,20))\n\nfor feature in df.columns:\n    plt.subplot(a, b, c)\n    sns.kdeplot(x = df[feature], fill=True, color=\"red\").set_title(f'{feature} Distribution ',fontsize=15)\n    c = c + 1\nplt.tight_layout(pad = 4.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:04.135267Z","iopub.execute_input":"2021-09-27T11:08:04.135535Z","iopub.status.idle":"2021-09-27T11:08:06.535903Z","shell.execute_reply.started":"2021-09-27T11:08:04.135489Z","shell.execute_reply":"2021-09-27T11:08:06.535179Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>Most Features are Not Normally Distrubuted</font>\n* <font color = \"red\" size=4.8px>Thus we will Impute The Data Accordingly</font>\n* <font color = \"red\" size=4.8px>Also We Will use Non-Linear Algorithms which doesnt expects Normal Distribution, for eg :- Tree based ensemble Techniques </font>    ","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=5px><b>2.4 Missing Value Imputation</b></font>","metadata":{}},{"cell_type":"code","source":"# Replacing Invalid Zeros with NaN Value For Imputation\n\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:06.537140Z","iopub.execute_input":"2021-09-27T11:08:06.537412Z","iopub.status.idle":"2021-09-27T11:08:06.549577Z","shell.execute_reply.started":"2021-09-27T11:08:06.537381Z","shell.execute_reply":"2021-09-27T11:08:06.548655Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Lets Check for Missing Values\n\npd.DataFrame({\"Missing Values\":df.isna().sum(),\"Percentage\":np.round(100*df.isna().sum()/df.shape[0],3)})","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:06.551130Z","iopub.execute_input":"2021-09-27T11:08:06.551475Z","iopub.status.idle":"2021-09-27T11:08:06.573020Z","shell.execute_reply.started":"2021-09-27T11:08:06.551433Z","shell.execute_reply":"2021-09-27T11:08:06.571634Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Lets Impute The Missing Values Using KNN Imputer\n\nknn_imputer= KNNImputer(n_neighbors=5)\n\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = knn_imputer.fit_transform(df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:06.574612Z","iopub.execute_input":"2021-09-27T11:08:06.575072Z","iopub.status.idle":"2021-09-27T11:08:06.635002Z","shell.execute_reply.started":"2021-09-27T11:08:06.575025Z","shell.execute_reply":"2021-09-27T11:08:06.633717Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# CHecking for Missing Values\n\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:06.636835Z","iopub.execute_input":"2021-09-27T11:08:06.637448Z","iopub.status.idle":"2021-09-27T11:08:06.660669Z","shell.execute_reply.started":"2021-09-27T11:08:06.637374Z","shell.execute_reply":"2021-09-27T11:08:06.659589Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=5px><b>2.5 Looking For Outliers</b></font>","metadata":{}},{"cell_type":"code","source":"style.use('ggplot')\n\na = 3  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\nplt.figure(figsize=(30,20))\n\nfor feature in df.columns:\n    plt.subplot(a, b, c)\n    sns.boxplot(x = df[feature]).set_title(f'{feature} ',fontsize=15)\n    c = c + 1\nplt.tight_layout(pad = 4.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:06.662176Z","iopub.execute_input":"2021-09-27T11:08:06.662743Z","iopub.status.idle":"2021-09-27T11:08:08.215573Z","shell.execute_reply.started":"2021-09-27T11:08:06.662696Z","shell.execute_reply":"2021-09-27T11:08:08.214578Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>Insulin and DiabetesPedigreeFunction shows high amount of Outliers</font>\n* <font color = \"red\" size=4.8px>better to use Non-Linear Approach such as  Spearman Correlation as its robust to outliers and using Algorithms which Are not affected by Outliers </font>","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n<p>\n    \n<font color = \"maroon\" size=5px><b>2.6 plot describing the data types and the count of variables</b></font>","metadata":{}},{"cell_type":"code","source":"df.dtypes.value_counts().plot(kind='bar')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:08.217041Z","iopub.execute_input":"2021-09-27T11:08:08.218159Z","iopub.status.idle":"2021-09-27T11:08:08.384589Z","shell.execute_reply.started":"2021-09-27T11:08:08.218110Z","shell.execute_reply":"2021-09-27T11:08:08.383625Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>No_of Integer Columns :- 6</font>\n* <font color = \"red\" size=4.8px>No_of Float Columns :- 3</font>","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n<p>\n    \n<font color = \"maroon\" size=5px><b>2.7 Lets Visualize Correlation Between Each Features</b></font>","metadata":{}},{"cell_type":"code","source":"style.use('ggplot')\n\na = 4  # number of rows\nb = 2  # number of columns\nc = 1  # initialize plot counter\nplt.figure(figsize=(30,30))\n\ni = 0\nfor i in range(0, df.columns.nunique()-1):\n    plt.subplot(a, b, c)\n    sns.scatterplot(x = df.iloc[:,i], y =df.iloc[:,i+1] ).set_title(f'Correlation of {df.iloc[:,i].name} with {df.iloc[:,i+1].name}',fontsize=20)\n    c = c + 1\nplt.tight_layout(pad = 4.0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:08.389416Z","iopub.execute_input":"2021-09-27T11:08:08.389722Z","iopub.status.idle":"2021-09-27T11:08:10.633480Z","shell.execute_reply.started":"2021-09-27T11:08:08.389690Z","shell.execute_reply":"2021-09-27T11:08:10.632736Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>Body Mass Index and Insulin Shows some Positive Correlation, which is Obvious, Higher the body Mass higher the blood Insulin </font>\n* <font color = \"red\" size=4.8px>Insulin and Skin Thickness hsows some Moderate to low Positive Correlation which makes sense as Isnulin tends to increase with BMI and BMI is directly Proportional to Skin Thickness</font>\n","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n<p>\n    \n<font color = \"maroon\" size=5px><b>2.8 Correlation Analysis Via Heat Map</b></font>","metadata":{}},{"cell_type":"code","source":"# Using Spearman Correlation as Data Contains Outliers\n\nplt.figure(figsize=(20,10))\nsns.heatmap(df.corr(method=\"spearman\"), annot=True)\nplt.title(\"Correlation Heat Map\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:10.634474Z","iopub.execute_input":"2021-09-27T11:08:10.635226Z","iopub.status.idle":"2021-09-27T11:08:11.469880Z","shell.execute_reply.started":"2021-09-27T11:08:10.635179Z","shell.execute_reply":"2021-09-27T11:08:11.468826Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"df.corr().style.background_gradient(cmap='viridis')","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.471229Z","iopub.execute_input":"2021-09-27T11:08:11.471507Z","iopub.status.idle":"2021-09-27T11:08:11.801448Z","shell.execute_reply.started":"2021-09-27T11:08:11.471474Z","shell.execute_reply":"2021-09-27T11:08:11.800546Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>There is High Correlation Among Features, but very less Correlation between Features and Target </font>\n\n","metadata":{}},{"cell_type":"markdown","source":"<hr>\n\n<p>\n    \n<font color = \"maroon\" size=5px><b>2.9 Lets Check If Data Is Balanced Or Imbalanced</b></font>","metadata":{}},{"cell_type":"code","source":"df.Outcome.value_counts(normalize=True).plot(kind='bar',color=\"royalblue\")\nplt.title(\"Proportion Of Outcome\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.803028Z","iopub.execute_input":"2021-09-27T11:08:11.803269Z","iopub.status.idle":"2021-09-27T11:08:11.963682Z","shell.execute_reply.started":"2021-09-27T11:08:11.803240Z","shell.execute_reply":"2021-09-27T11:08:11.962593Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=4.8px> <b>Interpretation : </b></font>\n\n* <font color = \"red\" size=4.8px>65% - 35% Proportion Indicated Data is Imbalanced and Hence accuracy wont be a Good Measure , we will Look at Sensitivity, Specificity, AUC(ROC curve) </font>\n* <font color = \"red\" size=4.8px>Also Tree Based  Algorithms Handles Imbalanced Data Pretty Well</font>\n","metadata":{}},{"cell_type":"markdown","source":"<hr>\n    \n<font color = \"blue\" size=6px><b><center>3. Devicing Strategy For Model Building</b></font>","metadata":{}},{"cell_type":"markdown","source":"* <p style = \"color:b; font-size:150%; font-weight:bold; text-decoration: underline; text-decoration-style: double;\"> So Far We have Arrived with Following Analysis : - </p>   \n    \n \n* Data is Skewed and has Lots of Outliers\n* Data is Imbalanced\n* Features Are Having Low Correlation with Target Variable\n* No of Samples Are too low So We Cannot Use Hold Out Method Of Cross Validation, we need to use Kfolds And Also Class Proportion is Imbalanced so We need to Use Stratified Kfold For Our Cross Validation Analysis With Minimum 10 FOLDs\n\n\n* <p style = \"color:r; font-size:120%; font-weight:bold\"> By these Inferences Its Evident that we should Use Tree Based Ensemble Techniques Which Can Handle Outliers, Skewed Data, Data Imbalances and Can Learn From Weak Learners","metadata":{}},{"cell_type":"markdown","source":"[[[[[[[[[[[](http://)](http://)](http://)](http://)](http://)](http://)](http://)](http://)](http://)](http://)](http://)<hr>\n    \n<font color = \"blue\" size=7px><b><center>4. Model Building</b></font>","metadata":{}},{"cell_type":"code","source":"# Kfold\n\nkf = StratifiedKFold(n_splits=10, shuffle=True, random_state=101)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.965060Z","iopub.execute_input":"2021-09-27T11:08:11.965294Z","iopub.status.idle":"2021-09-27T11:08:11.970168Z","shell.execute_reply.started":"2021-09-27T11:08:11.965265Z","shell.execute_reply":"2021-09-27T11:08:11.969087Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"blue\" size=6px><b><center> 4.1. K-Nearest-Neighbors</b></font>","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px><b>4.1.1 Standardization</b></font>\n\n\n- <p style = \"color:red; font-size:120%; font-weight:bold\">Standardization is Necessary while Dealing with Algorithms Which works on Euclidean distances such as KNN</p>\n- <p style = \"color:red; font-size:120%; font-weight:bold\">As there are Outliers In Data Set we Will Use RobustScalar As it is robust to Outliers</p>","metadata":{}},{"cell_type":"code","source":"Preprocessing = Pipeline(steps =[(\"Scaler\",RobustScaler()),(\"KNN\",KNeighborsClassifier(n_jobs=-1))])","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.971242Z","iopub.execute_input":"2021-09-27T11:08:11.971481Z","iopub.status.idle":"2021-09-27T11:08:11.982839Z","shell.execute_reply.started":"2021-09-27T11:08:11.971441Z","shell.execute_reply":"2021-09-27T11:08:11.981687Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Separating Features And Target\n\nFeatures = df.iloc[:,0:-1]\nTarget = df.iloc[:,-1]","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.984326Z","iopub.execute_input":"2021-09-27T11:08:11.984598Z","iopub.status.idle":"2021-09-27T11:08:11.995274Z","shell.execute_reply.started":"2021-09-27T11:08:11.984562Z","shell.execute_reply":"2021-09-27T11:08:11.994382Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Defining Parameter Space\n\ngrid_param = {'KNN__n_neighbors': optuna.distributions.IntUniformDistribution(1,20,1),\n             'KNN__weights': optuna.distributions.CategoricalDistribution(['uniform','distance']),\n             'KNN__metric': optuna.distributions.CategoricalDistribution(['minkowski','euclidean','manhattan'])}","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:11.996233Z","iopub.execute_input":"2021-09-27T11:08:11.996469Z","iopub.status.idle":"2021-09-27T11:08:12.009058Z","shell.execute_reply.started":"2021-09-27T11:08:11.996440Z","shell.execute_reply":"2021-09-27T11:08:12.008163Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Bayesian Hyperparameter Optimization Using Optuna\n\noptuna_search =  OptunaSearchCV(estimator=Preprocessing, param_distributions = grid_param,\n                                cv = kf, n_jobs = -1, n_trials=100, random_state = 101, refit = True,\n                                scoring = 'roc_auc', verbose = 0)\n\noptuna_search.fit(Features, Target)\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:12.010192Z","iopub.execute_input":"2021-09-27T11:08:12.010420Z","iopub.status.idle":"2021-09-27T11:08:51.959975Z","shell.execute_reply.started":"2021-09-27T11:08:12.010390Z","shell.execute_reply":"2021-09-27T11:08:51.958695Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"print(optuna_search.best_estimator_)\nprint(optuna_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:51.961550Z","iopub.execute_input":"2021-09-27T11:08:51.961799Z","iopub.status.idle":"2021-09-27T11:08:51.971630Z","shell.execute_reply.started":"2021-09-27T11:08:51.961768Z","shell.execute_reply":"2021-09-27T11:08:51.970511Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Study History \n\noptuna.visualization.plot_optimization_history(optuna_search.study_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:51.972876Z","iopub.execute_input":"2021-09-27T11:08:51.973474Z","iopub.status.idle":"2021-09-27T11:08:52.157887Z","shell.execute_reply.started":"2021-09-27T11:08:51.973442Z","shell.execute_reply":"2021-09-27T11:08:52.156656Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Plotting Best Parametrs\n\noptuna.visualization.plot_param_importances(optuna_search.study_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:52.159343Z","iopub.execute_input":"2021-09-27T11:08:52.159596Z","iopub.status.idle":"2021-09-27T11:08:53.452533Z","shell.execute_reply.started":"2021-09-27T11:08:52.159567Z","shell.execute_reply":"2021-09-27T11:08:53.451556Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"# Plotting Paramets Range\n\noptuna.visualization.plot_slice(optuna_search.study_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:53.454111Z","iopub.execute_input":"2021-09-27T11:08:53.454478Z","iopub.status.idle":"2021-09-27T11:08:53.751913Z","shell.execute_reply.started":"2021-09-27T11:08:53.454433Z","shell.execute_reply":"2021-09-27T11:08:53.750824Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Running The Model With Best Parameters\n\nbest_knn_model  = optuna_search.best_estimator_\n\n# Training model\n\nbest_knn_model.fit(Features,Target)\n\n# Calculating Cross Validation Score\n\nKnn_predictions = model_selection.cross_val_predict(best_knn_model,Features, Target,cv = kf, n_jobs = -1)\n\nprint(\"\\n KNN AUC_ROC Score : \", metrics.roc_auc_score(Knn_predictions, Target),\"\\n\")\n\n# Classification Report\n\nprint(\"Classification Report : \\n\\n\" ,metrics.classification_report(Target, Knn_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:53.752916Z","iopub.execute_input":"2021-09-27T11:08:53.753157Z","iopub.status.idle":"2021-09-27T11:08:54.669426Z","shell.execute_reply.started":"2021-09-27T11:08:53.753128Z","shell.execute_reply":"2021-09-27T11:08:54.668682Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"red\" size=6px><b><center> 4.2.XGboost</b></font>","metadata":{}},{"cell_type":"code","source":"# # Defining A Objecting Function For Hyper Paramater Tuning\n\n\n# def objective_xgb(trial, X, y, early_stopping_rounds):\n\n    \n#     # Param Space\n    \n#     params = {\n        \n#         \"verbosity\" : 0,\n#         'eval_metric' : 'auc',\n#         #'tree_method':'gpu_hist',# Use GPU acceleration\n#         #'predictor': 'gpu_predictor',\n#         \"seed\": 101,\n#         'n_jobs': -1,\n#         \"alpha\": trial.suggest_loguniform(\"alpha\", 1e-8, 1.0),\n#         \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 100.0),\n#         \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 100.0),\n#         \"colsample_bytree\": trial.suggest_loguniform(\"colsample_bytree\", 0.5, 0.8),\n#         \"subsample\": trial.suggest_loguniform(\"subsample\", 0.5, 0.8),\n#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.0001, 0.1),\n#         'n_estimators': 10000,\n#         'max_depth': trial.suggest_int(\"max_depth\", 1,80,log = True),\n#         \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 3, 100)\n#     }\n    \n    \n#     # Call Back For pruning unpromising trails\n#     pruning_callback = optuna.integration.XGBoostPruningCallback(trial, \"validation_0-auc\")\n#     model = XGBClassifier(**params)\n#     Metrics = [] # To store the AUC Scores\n    \n#     # Cross_validation Loop:\n    \n#     for train_index, test_index in kf.split(X,y):\n        \n#         # Setting Training and Testing Sets For Each Folds\n#         x_train, x_test = Features.iloc[train_index, :], Features.iloc[test_index, :]\n#         y_train, y_test = Target.iloc[train_index], Target.iloc[test_index]\n        \n#         # Fitting The Model on Each Fold\n#         model.fit(x_train, y_train,\n#                  eval_set = [(x_test, y_test)],\n#                  eval_metric = 'auc',\n#                  verbose = 0,\n#                  callbacks = [pruning_callback],\n#                  early_stopping_rounds = early_stopping_rounds\n#                  )\n        \n#         # Prediction for Each Fold\n#         y_pred = model.predict(x_test)\n        \n#         # Appending AUC Scores For Each Test Fold\n#         Metrics.append(metrics.roc_auc_score(y_test,y_pred))\n        \n#     return(np.mean(Metrics))\n\n\n# # Creating Optuna Study\n# study_xgb = optuna.create_study(direction=\"maximize\")\n\n# study_xgb.optimize(lambda trial : objective_xgb(trial, X=Features, y=Target, early_stopping_rounds=400),\n#                    n_trials=200, n_jobs=-1)\n\n\n# print(\"Best Value : \",study_xgb.best_value)\n# print(\"\\n\\n Best Params : \",study_xgb.best_params)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:54.670602Z","iopub.execute_input":"2021-09-27T11:08:54.670957Z","iopub.status.idle":"2021-09-27T11:08:54.677083Z","shell.execute_reply.started":"2021-09-27T11:08:54.670927Z","shell.execute_reply":"2021-09-27T11:08:54.676064Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Storing The Best Params\n\nbest_params = {'alpha': 1.7857171526334645e-07, 'lambda': 4.969111661609928e-06, 'gamma': 7.0867106981287455e-06,\n                'colsample_bytree': 0.7440989368070011, 'subsample': 0.5628507665726754,\n                'learning_rate': 0.004267786629571522, 'max_depth': 4, 'min_child_weight': 3.082113258455785}\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:54.678266Z","iopub.execute_input":"2021-09-27T11:08:54.678599Z","iopub.status.idle":"2021-09-27T11:08:54.694096Z","shell.execute_reply.started":"2021-09-27T11:08:54.678568Z","shell.execute_reply":"2021-09-27T11:08:54.693114Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Final Prediction and Validation for XGBoost\n\ntemp = {'verbosity' : 0, 'eval_metric' : 'auc',\n        \"seed\": 101,'n_jobs': -1,'n_estimators': 10000}\n\nbest_params.update(temp)\n\n\n# Model with best Params\nmodel_xgb = XGBClassifier(**best_params)\n\nXgb_predictions = np.zeros(len(Target.values))\n\nXgb_predictions = pd.DataFrame(Xgb_predictions)\n\n# Cross_validation Loop:\n\n\n\nfor train_index, test_index in kf.split(Features,Target):\n        \n    # Setting Training and Testing Sets For Each Folds\n    x_train, x_test = Features.iloc[train_index,:], Features.iloc[test_index,:]\n    y_train, y_test = Target.iloc[train_index], Target.iloc[test_index]\n\n    # Fitting The Model on Each Fold\n    model_xgb.fit(x_train, y_train,\n                eval_set = [(x_test, y_test)],\n                verbose = 0,\n                early_stopping_rounds = 100\n                )\n\n    # Prediction for Each Fold\n    Xgb_predictions.iloc[test_index,0] = model_xgb.predict(x_test)\n     ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:54.695558Z","iopub.execute_input":"2021-09-27T11:08:54.695911Z","iopub.status.idle":"2021-09-27T11:08:55.943130Z","shell.execute_reply.started":"2021-09-27T11:08:54.695857Z","shell.execute_reply":"2021-09-27T11:08:55.942212Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"print(\"Best Number of Trees(estimators)\",model_xgb.best_ntree_limit)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:55.944649Z","iopub.execute_input":"2021-09-27T11:08:55.945003Z","iopub.status.idle":"2021-09-27T11:08:55.951676Z","shell.execute_reply.started":"2021-09-27T11:08:55.944946Z","shell.execute_reply":"2021-09-27T11:08:55.950420Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"print(\"AUC_ROC Score : \",metrics.roc_auc_score(Target,Xgb_predictions))\nprint(\"\\n Classification Report : \\n\\n\",metrics.classification_report(Target,Xgb_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:55.953723Z","iopub.execute_input":"2021-09-27T11:08:55.954338Z","iopub.status.idle":"2021-09-27T11:08:55.978794Z","shell.execute_reply.started":"2021-09-27T11:08:55.954288Z","shell.execute_reply":"2021-09-27T11:08:55.977704Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"red\" size=6px><b><center> 4.3.LightGBM</b></font>","metadata":{}},{"cell_type":"code","source":"# # Defining A Objecting Function For Hyper Paramater Tuning\n\n\n# def objective_lgbm(trial, X, y, early_stopping_rounds):\n\n    \n#     # Param Space\n    \n#     params = {\n#         \"seed\":101,\n#         #\"verbosity\":-1,\n#         \"save_binary\":True,\n#         \"num_threads\":4,\n#         \"boosting\":\"gbdt\",\n#         \"extra_trees\":True,\n#         \"metric\":\"auc\",\n#         #\"xgboost_dart_mode\":trial.suggest_categorical(\"xgboost_dart_mode\", [True,False]),\n#         \"is_unbalance\":True,\n#         #\"device_type\": trial.suggest_categorical(\"device_type\", ['gpu']),\n#         \"n_estimators\":10000,\n#         \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.1, 1.0),\n#         \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 2500, step=10),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20, step = 1),\n#         \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 1,100,step=2),\n#         \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.00001, 4, step = 0.1),\n#         \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.001, 20.0, step = 0.1),\n#         \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 1, 9),\n#         \"bagging_fraction\" : 0.95,\n#         \"feature_fraction\" : 0.95,\n#         \"bagging_freq\" : 5, \n#         \"bagging_seed\" : 101\n#     }\n    \n    \n#     # Call Back For pruning unpromising trails\n#     pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n#     model = LGBMClassifier(**params, objective=\"binary\",silent=True,verbose=-100)\n#     Metrics = [] # To store the AUC Scores\n    \n#     # Cross_validation Loop:\n    \n#     for train_index, test_index in kf.split(X,y):\n        \n#         # Setting Training and Testing Sets For Each Folds\n#         x_train, x_test = Features.iloc[train_index, :], Features.iloc[test_index, :]\n#         y_train, y_test = Target.iloc[train_index], Target.iloc[test_index]\n        \n#         # Fitting The Model on Each Fold\n#         model.fit(x_train, y_train,\n#                  eval_set = [(x_test, y_test)],\n#                  eval_metric = \"auc\",\n#                  verbose = False,\n#                  callbacks = [pruning_callback],\n#                  early_stopping_rounds = early_stopping_rounds\n#                  )\n        \n#         # Prediction for Each Fold\n#         y_pred = model.predict(x_test)\n        \n#         # Appending AUC Scores For Each Test Fold\n#         Metrics.append(metrics.roc_auc_score(y_test,y_pred))\n        \n#     return(np.mean(Metrics))\n\n\n\n# # Creating Optuna Study\n# study_lgbm = optuna.create_study(direction=\"maximize\")\n\n# study_lgbm.optimize(lambda trial : objective_lgbm(trial, X=Features, y=Target, early_stopping_rounds=400),\n#                    n_trials=200, n_jobs=-1)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:55.980722Z","iopub.execute_input":"2021-09-27T11:08:55.981087Z","iopub.status.idle":"2021-09-27T11:08:55.988140Z","shell.execute_reply.started":"2021-09-27T11:08:55.981042Z","shell.execute_reply":"2021-09-27T11:08:55.986942Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"best_params = {'learning_rate': 0.4107999846346181, 'num_leaves': 2450, 'max_depth': 11, 'min_data_in_leaf': 11,\n               'lambda_l1': 0.20001000000000002, 'lambda_l2': 0.901, 'min_gain_to_split': 1.5352032889321992}\n","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:55.990237Z","iopub.execute_input":"2021-09-27T11:08:55.990624Z","iopub.status.idle":"2021-09-27T11:08:56.005621Z","shell.execute_reply.started":"2021-09-27T11:08:55.990578Z","shell.execute_reply":"2021-09-27T11:08:56.004651Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Final Prediction and Validation for LightGBM\n\ntemp = {'verbosity' : -100, 'metric' : 'auc', \"extra_trees\":True,\n        \"seed\": 101, \"is_unbalance\":True,'num_threads': 4 , 'n_estimators': 10000,\n       \"save_binary\":True, \"bagging_fraction\" : 0.95, \"feature_fraction\" : 0.95,\n        \"bagging_freq\" : 5, \"bagging_seed\" : 101}\n\nbest_params.update(temp)\n\n\n# Model with best Params\nmodel_lgbm = LGBMClassifier(**best_params, objective=\"binary\",silent=True)\n\nlgbm_predictions = np.zeros(len(Target.values))\n\nlgbm_predictions = pd.DataFrame(lgbm_predictions)\n\n# Cross_validation Loop:\n\n\n\nfor train_index, test_index in kf.split(Features,Target):\n        \n    # Setting Training and Testing Sets For Each Folds\n    x_train, x_test = Features.iloc[train_index,:], Features.iloc[test_index,:]\n    y_train, y_test = Target.iloc[train_index], Target.iloc[test_index]\n\n    # Fitting The Model on Each Fold\n    model_lgbm.fit(x_train, y_train,\n                eval_set = [(x_test, y_test)],\n                eval_metric = \"auc\",\n                verbose = False,\n                early_stopping_rounds = 400\n                )\n\n    # Prediction for Each Fold\n    lgbm_predictions.iloc[test_index,0] = model_lgbm.predict(x_test)\n    \nprint(\"\\n\\n Light GBM AUC_ROC Score : \", metrics.roc_auc_score(Target,lgbm_predictions))\nprint(\"\\n Classification Report : \\n\\n\",metrics.classification_report(Target,lgbm_predictions))    ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:56.007045Z","iopub.execute_input":"2021-09-27T11:08:56.007841Z","iopub.status.idle":"2021-09-27T11:08:57.429922Z","shell.execute_reply.started":"2021-09-27T11:08:56.007800Z","shell.execute_reply":"2021-09-27T11:08:57.428719Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"print(\"Best Number of Trees(estimators)\", model_lgbm.best_iteration_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:11:09.599037Z","iopub.execute_input":"2021-09-27T11:11:09.599331Z","iopub.status.idle":"2021-09-27T11:11:09.604420Z","shell.execute_reply.started":"2021-09-27T11:11:09.599301Z","shell.execute_reply":"2021-09-27T11:11:09.603676Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"red\" size=6px><b><center> 4.4.Random Forest</b></font>","metadata":{}},{"cell_type":"code","source":"# %%time\n\n\n\n# grid_param = {'n_estimators': optuna.distributions.IntUniformDistribution(220,3000,100),\n#                'max_features': optuna.distributions.CategoricalDistribution(['sqrt','auto','log2']),\n#               \"class_weight\" : optuna.distributions.CategoricalDistribution(['balanced_subsample']),\n#                \"bootstrap\": optuna.distributions.CategoricalDistribution([True,False]),\n#                'max_depth': optuna.distributions.IntLogUniformDistribution(2,50,1),\n#                'min_samples_split': optuna.distributions.IntUniformDistribution(2,10),\n#                'min_samples_leaf': optuna.distributions.IntUniformDistribution(1,10),\n#                'criterion': optuna.distributions.CategoricalDistribution([\"gini\", \"entropy\"])}\n\n# optuna_search = OptunaSearchCV(estimator=RandomForestClassifier(random_state=101, n_jobs=-1,warm_start=True), param_distributions = grid_param,\n#                                cv = kf, n_jobs = -1, n_trials=100, random_state = 101, refit = True,\n#                                scoring = 'roc_auc', verbose = 3)\n\n# optuna_search.fit(Features, Target)\n\n# print(optuna_search.best_estimator_)\n\n# print(optuna_search.best_score_)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:08:57.639998Z","iopub.status.idle":"2021-09-27T11:08:57.640395Z","shell.execute_reply.started":"2021-09-27T11:08:57.640186Z","shell.execute_reply":"2021-09-27T11:08:57.640205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating Model Object\nrfModel = RandomForestClassifier(bootstrap=False, class_weight='balanced_subsample',\n                       criterion='entropy', max_depth=5, max_features='sqrt',\n                       min_samples_leaf=8, min_samples_split=6,\n                       n_estimators=320, n_jobs=-1, random_state=101,\n                       warm_start=True)\n\nrfModel.fit(Features,Target)\n\n\nrfModel_predictions = model_selection.cross_val_predict(rfModel,Features,Target,cv=kf,n_jobs=-1)\n\n# Cross Validation Score(Accuracy)\n\nprint(\"\\n\\n Random Forest AUC_ROC Score : \", metrics.roc_auc_score(Target,rfModel_predictions))\nprint(\"\\n Classification Report : \\n\\n\",metrics.classification_report(Target,rfModel_predictions))  ","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:11:13.556225Z","iopub.execute_input":"2021-09-27T11:11:13.556706Z","iopub.status.idle":"2021-09-27T11:11:18.425325Z","shell.execute_reply.started":"2021-09-27T11:11:13.556656Z","shell.execute_reply":"2021-09-27T11:11:18.424299Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"red\" size=6px><b><center> 5. Model Evaluation </b></font>","metadata":{}},{"cell_type":"markdown","source":"\n\n\n- <p style = \"color:maroon; font-size:200%; font-weight:bold\">Senstivity </p>\n- <font size=4px color=blue ><b>Senstivity Aka True Positive Rate(Recall) : - </b> Out of Total Actual Positive Patiens ,How Many Did we Correctly Predicted to be Diabetes Positive\n    \n- <font size=4px color=blue ><b>Precision : - </b> Out of Total Predicted Positive Patiens ,How Many were Actual Diabetes Positive\n​\n\n- <p style = \"color:maroon; font-size:200%; font-weight:bold\">Specificity</p>\n- <font size=4px color=blue ><b>Specificity Aka False Positive Rate : - </b> Out of Total Actual Positive Patiens ,How Many Did we Incorrectly Predicted to be Diabetes Positive\n    \n    \n    \n<hr>","metadata":{}},{"cell_type":"markdown","source":"- <font size=4.8px color=b><b>A good Model is Who's Senstivity > Specificity <br>we take Following Points For Evaluation of Model Depending of Our Problem Statement and The data :-</font>\n    \n    - <font size=4.8px color=red> <b>False Negative</b></font><br> <font size = 4px wieghtbold>i.e Number of Patients Incorrectly Classified as Non-Diabeic, this is critical since Patients got Incorrectly Predicted to be Non-Diabitic, but they were Diabetic In Actual, They will not Get the treatment and thus results can be fatal, to Healtcare Company as well as to Patients\n    \n    - <font size=4.8px color=red> <b>False Positive</b></font><br> <font size = 4px wieghtbold>i.e Number of Patients Incorrectly Classified as Diabetic, this is critical since Patients got Incorrectly Predicted to be Diabitic, but they were Non-Diabetic In Actual, They will be trated as Diabetic Patients and the Steroids can result in fatal Side effects\n    \n    - <font size = 4px wieghtbold> Thus <b>False Positive</b> and <b>False Negative</b> are Equally Critical, i.e , <b>Precision</b> and <b>Recall</b> are Eqaully Important \n    \n    - <font size = 4px wieghtbold> So we Measure The Model Performance With <b>F-Beta Score \n    \n    \n<hr>","metadata":{}},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px><b>5.1 Storing The F1-Score of All Models</b></font>","metadata":{}},{"cell_type":"code","source":"# F1 Scores\n\nKnn_F1_Score = metrics.f1_score(Target,Knn_predictions)\nXGB_F1_Score = metrics.f1_score(Target,Xgb_predictions)\nLGBM_F1_Score = metrics.f1_score(Target,lgbm_predictions)\nRF_F1_Score = metrics.f1_score(Target,rfModel_predictions)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:11:18.426920Z","iopub.execute_input":"2021-09-27T11:11:18.427184Z","iopub.status.idle":"2021-09-27T11:11:18.440998Z","shell.execute_reply.started":"2021-09-27T11:11:18.427154Z","shell.execute_reply":"2021-09-27T11:11:18.439790Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px><b>5.2 Classification report of All Model</b></font>","metadata":{}},{"cell_type":"code","source":"print(\"Classification Report KNN: \\n\\n\", metrics.classification_report(Target,Knn_predictions),end=\"\")\nprint(\"\\n\\nClassification Report XGB: \\n\\n\", metrics.classification_report(Target,Xgb_predictions))\nprint(\"\\n\\nClassification Report LGBM: \\n\\n\", metrics.classification_report(Target,lgbm_predictions))\nprint(\"\\n\\nClassification Report RF: \\n\\n\", metrics.classification_report(Target,rfModel_predictions))","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:11:18.789870Z","iopub.execute_input":"2021-09-27T11:11:18.790193Z","iopub.status.idle":"2021-09-27T11:11:18.825294Z","shell.execute_reply.started":"2021-09-27T11:11:18.790161Z","shell.execute_reply":"2021-09-27T11:11:18.824229Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Creating A Data Frame Wwith Models And their F1-Scores\n\n(pd.DataFrame({\"Model\":[\"KNN\", \"XGboost\", \"LGBM\", \"Random_Forest\"],\n \"F1-Score\" : [Knn_F1_Score, XGB_F1_Score, LGBM_F1_Score, RF_F1_Score]})).sort_values(by=\"F1-Score\", ascending=False)","metadata":{"execution":{"iopub.status.busy":"2021-09-27T11:11:19.008263Z","iopub.execute_input":"2021-09-27T11:11:19.009204Z","iopub.status.idle":"2021-09-27T11:11:19.023916Z","shell.execute_reply.started":"2021-09-27T11:11:19.009140Z","shell.execute_reply":"2021-09-27T11:11:19.022823Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"<font color = \"maroon\" size=4px><b>Conclusion</b></font>\n\n\n- <p style = \"color:red; font-size:120%; font-weight:bold\">Best Performing Model In terms of F1-Score is LGBM, Because it has Ability to Deal with Imbalanced Data Set Better than any Model</p>\n- <p style = \"color:red; font-size:120%; font-weight:bold\">Knn turns out to be worst of all, Because it wasnt suitable to data as it is Linear Model and Our data is Non-Linear </p>","metadata":{}}]}